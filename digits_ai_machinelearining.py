# -*- coding: utf-8 -*-
"""Digits Copy of AI_Project2_MachineLearining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1otj-OWf9gcVA44r5Y9CqdDwZAl4KkMEP

**Imports and Fetching data from digits Dataset**
"""

from numpy import mean
from numpy import std
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

"""**Convert X_train to zero mean and variance of 1 to be easier to use by using the PCA(Principal Component Analysis) to fit the X_train which reduce the variable numbers to smaller number to be easier and faster to deal with, whiten is used to make input less redundent**"""

# performing preprocessing part
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = X_train.reshape(len(X_train), -1) #Converting 2d array to Vector
X_train = sc.fit_transform(X_train)

X_test = X_test.reshape(len(X_test), -1) #Converting 2d array to Vector
X_test = sc.transform(X_test)

# Compute a PCA 
n_components = 75
pca = PCA(n_components=n_components, whiten=True).fit(X_train)
#n_components -> principal components used in dimensionality reduction
#whiten -> it is needed for some algorithms. If we are training on images, the raw input is redundant, 
#since adjacent pixel values are highly correlated. The goal of whitening is to make the input less redundant
# apply PCA transformation
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

"""**KNN is used which gave an accuracy nearly equal to 0.95**"""

from sklearn.neighbors import KNeighborsClassifier
print("Fitting the classifier to the training set")
clf = KNeighborsClassifier(n_neighbors=6).fit(X_train_pca, y_train)
# from sklearn.naive_bayes import GaussianNB
# clf= GaussianNB().fit(X_train, y_train)
# clf= GaussianNB().fit(X_train_pca, y_train)
#1e-9 -> default variance so when decreasing it , accuracy decreases

"""**Using test in prediction and printing a classification report and an accuracy score**"""

y_pred = clf.predict(X_test_pca)
print(metrics.classification_report(y_test, y_pred))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""**KFold is used to cross so that the test data and trained data are merged and the naive bayes model is used**"""

for i in range(5,20):
  # cv = KFold(n_splits=i, random_state=1, shuffle=True)
  cv = KFold(n_splits=i)
  scores = cross_val_score(clf, X_train_pca, y_train, scoring='accuracy', cv=cv)
  # report performance
  print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))